{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "def dataset_tensor_to_np(tensor, dtype='image'):\n",
    "    assert dtype in ['image', 'events', 'seg']\n",
    "    output = None\n",
    "    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    cityscapes_color_list = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153,\n",
    "                             250, 170, 30, 220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60,\n",
    "                             255, 0, 0, 0, 0, 142, 0, 0, 70, 0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n",
    "    if dtype == 'image':\n",
    "        tensor = tensor.cpu().numpy()\n",
    "        for index in range(tensor.shape[0]):\n",
    "            tensor[index, :, :] = tensor[index, :, :] * mean_std[1][index] + mean_std[0][index]\n",
    "        output = np.uint8(np.transpose(tensor * 255, (1, 2, 0)))  # (0, 1)\n",
    "    elif dtype == 'events':\n",
    "        tensor = tensor.cpu().numpy()\n",
    "        if tensor.shape[0] == 3:\n",
    "            tensor = np.uint8((tensor + 1) / 2 * 255)\n",
    "            output = np.transpose(tensor, (1, 2, 0))\n",
    "        else:\n",
    "            tensor = np.uint8((tensor[0] + 1) / 2 * 255)\n",
    "            tensor = np.expand_dims(tensor, axis=0).repeat(3, axis=0)\n",
    "            output = np.transpose(tensor, (1, 2, 0))\n",
    "    elif dtype == 'seg':\n",
    "        if tensor.shape[0] != 1:\n",
    "            tensor = torch.argmax(tensor, dim=0)\n",
    "        else:\n",
    "            tensor = tensor[0]\n",
    "        tensor = tensor.cpu().numpy()\n",
    "        tensor = Image.fromarray(tensor.astype(np.uint8)).convert('P')\n",
    "        tensor.putpalette(cityscapes_color_list)\n",
    "        output = np.uint8(np.array(tensor.convert('RGB')))\n",
    "    return output\n",
    "\n",
    "\n",
    "def tensor_normalize_to_range(tensor, min_val, max_val):\n",
    "    tensor_min = torch.min(tensor)\n",
    "    tensor_max = torch.max(tensor)\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min + 1e-8) * (max_val - min_val) + min_val\n",
    "    return tensor\n",
    "\n",
    "def get_ic(image_front, image_now, val_range=(0, 255), threshold=0.1, clip_range=0.3):\n",
    "    image_front = np.asarray(image_front, dtype=np.float32)\n",
    "    image_front = np.log(image_front / 255 * (val_range[1] - val_range[0]) + val_range[0])\n",
    "    image_now = np.asarray(image_now, dtype=np.float32)\n",
    "    image_now = np.log(image_now / 255 * (val_range[1] - val_range[0]) + val_range[0])\n",
    "    image_change_ = torch.unsqueeze(torch.from_numpy(image_now - image_front), dim=0)\n",
    "    threshold = (np.log(val_range[1]) - np.log(val_range[0])) * threshold\n",
    "    clip_range = (np.log(val_range[1]) - np.log(val_range[0])) * clip_range\n",
    "    mask = (torch.abs(image_change_) <= threshold)\n",
    "    image_change_[mask] = 0\n",
    "    image_change_smaller_0 = image_change_.detach().clone()\n",
    "    image_change_[image_change_ < 0] = 0\n",
    "    image_change_ = torch.clamp(image_change_, 0, clip_range)\n",
    "    image_change_ = tensor_normalize_to_range(image_change_, min_val=0, max_val=1)\n",
    "    image_change_smaller_0[image_change_smaller_0 > 0] = 0\n",
    "    image_change_smaller_0 = torch.clamp(image_change_smaller_0, -clip_range, 0)\n",
    "    image_change_smaller_0 = tensor_normalize_to_range(image_change_smaller_0, min_val=-1, max_val=0)\n",
    "    image_change_ += image_change_smaller_0\n",
    "    return image_change_\n",
    "\n",
    "\n",
    "def get_image_change_from_pil(pil_image, width, height, data_type, shift_pixel=4, val_range=None,\n",
    "                              _threshold=None, _clip_range=None):\n",
    "    assert data_type in {'day', 'night', 'new_day'}\n",
    "\n",
    "    if val_range is not None:\n",
    "        val_range = val_range\n",
    "    if _threshold is not None:\n",
    "        threshold = _threshold\n",
    "    if _clip_range is not None:\n",
    "        clip_range = _clip_range\n",
    "\n",
    "    inputs_gray = np.array(pil_image.convert('L'))  # (480, 640)\n",
    "    inputs_left = np.concatenate((inputs_gray[:, shift_pixel:], inputs_gray[:, width-shift_pixel:]), axis=1)\n",
    "    inputs_right = np.concatenate((inputs_gray[:, :shift_pixel], inputs_gray[:, :width-shift_pixel]), axis=1)\n",
    "    inputs_up = np.concatenate((inputs_gray[shift_pixel:, :], inputs_gray[height-shift_pixel:, :]), axis=0)\n",
    "    inputs_down = np.concatenate((inputs_gray[:shift_pixel, :], inputs_gray[:height-shift_pixel, :]), axis=0)\n",
    "    # image_change_1 = get_ic(inputs_gray, inputs_right, val_range=val_range, threshold=threshold, clip_range=clip_range)\n",
    "    # image_change_2 = get_ic(inputs_gray, inputs_down, val_range=val_range, threshold=threshold, clip_range=clip_range)\n",
    "    image_change_1 = get_ic(inputs_gray, inputs_up, val_range=val_range, threshold=threshold, clip_range=clip_range)\n",
    "    image_change_2 = get_ic(inputs_gray, inputs_left, val_range=val_range, threshold=threshold, clip_range=clip_range)\n",
    "\n",
    "    # image_change_3 = get_ic(inputs_gray, inputs_down, val_range=val_range, threshold=threshold, clip_range=clip_range)\n",
    "    # image_change_4 = get_ic(inputs_gray, inputs_right, val_range=val_range, threshold=threshold, clip_range=clip_range)\n",
    "\n",
    "    image_avg = image_change_1 / 2 + image_change_2 / 2\n",
    "    # image_avg = image_change_1 / 4 + image_change_2 / 4 + image_change_3 / 4 + image_change_4 / 4\n",
    "    return image_avg  # tensor -1 ~ +1\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 1, 480, 640]), min: -4.4760637283325195, max: 5.348857879638672\n",
      "tensor(2.8221)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "kirsch_kernels = [\n",
    "    torch.tensor([[ 5,  5,  5],\n",
    "                  [-3,  0, -3],\n",
    "                  [-3, -3, -3]], dtype=torch.float),\n",
    "    torch.tensor([[-3,  5,  5],\n",
    "                  [-3,  0,  5],\n",
    "                  [-3, -3, -3]], dtype=torch.float),\n",
    "    torch.tensor([[-3, -3,  5],\n",
    "                  [-3,  0,  5],\n",
    "                  [-3, -3,  5]], dtype=torch.float),\n",
    "    torch.tensor([[-3, -3, -3],\n",
    "                  [-3,  0,  5],\n",
    "                  [-3,  5,  5]], dtype=torch.float),\n",
    "    torch.tensor([[-3, -3, -3],\n",
    "                  [-3,  0, -3],\n",
    "                  [ 5,  5,  5]], dtype=torch.float),\n",
    "    torch.tensor([[-3, -3, -3],\n",
    "                  [ 5,  0, -3],\n",
    "                  [ 5,  5, -3]], dtype=torch.float),\n",
    "    torch.tensor([[ 5, -3, -3],\n",
    "                  [ 5,  0, -3],\n",
    "                  [ 5, -3, -3]], dtype=torch.float),\n",
    "    torch.tensor([[ 5,  5, -3],\n",
    "                  [ 5,  0, -3],\n",
    "                  [-3, -3, -3]], dtype=torch.float)\n",
    "]\n",
    "\n",
    "rho_kernels = [\n",
    "    torch.tensor([[ 3, -1],\n",
    "                  [-1, -1]], dtype=torch.float),\n",
    "    torch.tensor([[-1,  3],\n",
    "                  [-1, -1]], dtype=torch.float),\n",
    "    torch.tensor([[-1, -1],\n",
    "                  [-1,  3]], dtype=torch.float),\n",
    "    torch.tensor([[-1, -1],\n",
    "                  [ 3, -1]], dtype=torch.float),\n",
    "]\n",
    "\n",
    "# rho_kernels = [\n",
    "#     torch.tensor([[ 1,  0],\n",
    "#                   [ 0, -1]], dtype=torch.float),\n",
    "#     torch.tensor([[ 0,  1],\n",
    "#                   [-1,  0]], dtype=torch.float),\n",
    "#     torch.tensor([[-1,  0],\n",
    "#                   [ 0,  1]], dtype=torch.float),\n",
    "#     torch.tensor([[ 0, -1],\n",
    "#                   [ 1,  0]], dtype=torch.float),\n",
    "# ]\n",
    "\n",
    "diff_kernels = [\n",
    "    # torch.tensor([[ 1, 0],\n",
    "    #               [ 0, -1]], dtype=torch.float),\n",
    "    torch.tensor([[ 3, -1],\n",
    "                  [-1, -1]], dtype=torch.float),\n",
    "]\n",
    "\n",
    "class Kirsch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Kirsch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1, bias=False, padding_mode='reflect')\n",
    "        with torch.no_grad():\n",
    "            for i, kernel in enumerate(kirsch_kernels):\n",
    "                self.conv1.weight[i].copy_(kernel.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)\n",
    "\n",
    "class Rho(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rho, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=2, padding=1, bias=False, padding_mode='reflect')\n",
    "        for i, kernel in enumerate(rho_kernels):\n",
    "            self.conv1.weight[i].copy_(kernel.unsqueeze(0))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)\n",
    "\n",
    "\n",
    "class Diff(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Diff, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=2, padding=1, bias=False, padding_mode='reflect')\n",
    "        for i, kernel in enumerate(diff_kernels):\n",
    "            self.conv1.weight[i].copy_(kernel.unsqueeze(0))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)\n",
    "\n",
    "def print_tensor_shape_min_max(tensor):\n",
    "    print('shape: {}, min: {}, max: {}'.format(tensor.shape, torch.min(tensor), torch.max(tensor)))\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    gray = np.array(img.convert('L'))\n",
    "    gray_tensor = torch.from_numpy(gray).unsqueeze(0).unsqueeze(0).float() / 255.0  # 0~1\n",
    "    return gray_tensor, img\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def conv_operation_on_image(gray_tensor, kernel_type, min_parm=0.2, max_parm=0.999):\n",
    "    assert kernel_type in {'diff', 'rho', 'kirsch'}\n",
    "    if kernel_type == 'diff':\n",
    "        operation = Diff()\n",
    "    elif kernel_type == 'rho':\n",
    "        operation = Rho()\n",
    "    else:\n",
    "        operation = Kirsch()\n",
    "    edges_tensor = operation(torch.log((gray_tensor + 1e-2)))[:, :, :-1, :-1]\n",
    "    if kernel_type in {'rho', 'kirsch'}:\n",
    "        edges_tensor = torch.max(edges_tensor, dim=1, keepdim=True)[0]\n",
    "    print_tensor_shape_min_max(edges_tensor)\n",
    "    min_clip_thres = min_parm\n",
    "    if kernel_type == 'diff':\n",
    "        max_clip_thres = torch.quantile(edges_tensor[edges_tensor > 0], max_parm)  # edges_tensor[edges_tensor > 0]  torch.abs(edges_tensor)\n",
    "        print(max_clip_thres)\n",
    "        edges_tensor[torch.abs(edges_tensor) < min_clip_thres] = 0\n",
    "        edges = torch.clamp(edges_tensor, min=-max_clip_thres, max=max_clip_thres) / max_clip_thres\n",
    "        edges = (edges + 1) / 2\n",
    "    else:\n",
    "        max_clip_thres = torch.quantile(edges_tensor, max_parm)\n",
    "        edges = torch.clamp(edges_tensor, min=min_clip_thres, max=max_clip_thres)\n",
    "        edges = (edges - min_clip_thres) / (max_clip_thres - min_clip_thres)\n",
    "    return edges\n",
    "\n",
    "# from dsec_dataset import DSECDataset\n",
    "# dataset = DSECDataset(root_path='G:/DSEC_Dataset/', events_bins=20, events_clip_range=0.95, crop_size=(400, 400),\n",
    "#                       sequences_num=1, events_step=1, ignore_down_pixel=20, test_flag=False, test_seg=True)\n",
    "\n",
    "\n",
    "# image_path = r'G:\\DSEC_Dataset\\train_wrap_images\\zurich_city_06_a\\000000.png'\n",
    "image_path = r'E:\\DSEC_Dataset\\train_wrap_images\\zurich_city_05_b\\000016.png'\n",
    "# image_path = r'D:\\研究生\\Python\\HANet\\cityscapes\\leftImg8bit\\train\\cologne\\cologne_000037_000019_leftImg8bit.png'\n",
    "kernel_type = 'diff'  # diff rho kirsch\n",
    "min_parm = 0\n",
    "max_parm = 0.999\n",
    "\n",
    "gray_tensor, img = load_image(image_path)\n",
    "edges = conv_operation_on_image(gray_tensor, kernel_type, min_parm, max_parm)\n",
    "\n",
    "# mse_mean_loss = nn.MSELoss(reduction='mean')\n",
    "# mse_none_loss = nn.MSELoss(reduction='none')\n",
    "# edges_zeros = torch.zeros_like(edges)\n",
    "# edges_ones = torch.zeros_like(edges) + 1\n",
    "#\n",
    "# edges_max_weight = 50\n",
    "#\n",
    "# base_num = 3\n",
    "# edges_weight = 1 / (base_num - 1) * (base_num * (1 - edges_max_weight) * torch.pow(base_num, -edges) + base_num * edges_max_weight - 1)\n",
    "# zeros_loss = mse_none_loss(edges, edges_zeros) * edges_weight\n",
    "# zeros_loss = zeros_loss.sum() / edges.numel()\n",
    "#\n",
    "# ones_loss = mse_none_loss(edges, edges_ones) * edges_weight\n",
    "# ones_loss = ones_loss.sum() / edges.numel()\n",
    "#\n",
    "# print(torch.mean(edges))\n",
    "# print(zeros_loss, ones_loss)\n",
    "\n",
    "edges = np.uint8(edges[0, 0][:, :, None].repeat(1, 1, 3).numpy() * 255)\n",
    "output_show_image = np.concatenate((np.array(img), edges), axis=1)\n",
    "Image.fromarray(output_show_image).save(r'D:\\研究生\\总结\\2023.5.6总结\\2.png')\n",
    "Image.fromarray(output_show_image).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as standard_transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# mean_std = ([0.5, 0.5, 0.5], [0.229, 0.224, 0.225])\n",
    "\n",
    "mean_std = ([127, 127, 127], [58.395, 57.12, 57.375])\n",
    "img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "pil_to_tensor_transform = standard_transforms.Compose([standard_transforms.PILToTensor()])\n",
    "HorizontalFlip = standard_transforms.RandomHorizontalFlip(p=1)\n",
    "\n",
    "image_path = 'E:/dsec_dataset/train_edges/zurich_city_01_a/000024.png'\n",
    "edges_pil = Image.open(image_path).convert('RGB')\n",
    "\n",
    "edges_torch = pil_to_tensor_transform(edges_pil)\n",
    "edges_torch = edges_torch.to(torch.float32)\n",
    "edges_torch = standard_transforms.functional.normalize(edges_torch, mean_std[0], mean_std[1])\n",
    "\n",
    "edges_torch_flip = HorizontalFlip(edges_pil)\n",
    "\n",
    "print(type(edges_torch_flip))\n",
    "print(edges_torch.shape, torch.min(edges_torch), torch.max(edges_torch), edges_torch.dtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "print(tuple(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2]\n",
    "print(tuple(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}